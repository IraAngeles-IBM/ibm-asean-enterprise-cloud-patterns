{"componentChunkName":"component---src-pages-log-streaming-configure-streaming-for-third-party-tools-index-mdx","path":"/log-streaming/configure-streaming-for-third-party-tools/","result":{"pageContext":{"frontmatter":{"title":"Configure Streaming for Third Party Tools","description":"How does Log Streaming work & how to leverage it","keywords":"ibm cloud"},"relativePagePath":"/log-streaming/configure-streaming-for-third-party-tools/index.mdx","titleType":"page","MdxNode":{"id":"7f33147d-28ba-5ce5-a93d-f2fffca208e4","children":[],"parent":"6804335b-631c-5935-9b47-5cf5770b7747","internal":{"content":"---\ntitle: Configure Streaming for Third Party Tools\ndescription: How does Log Streaming work & how to leverage it\nkeywords: 'ibm cloud'\n---\n\nIn this document, We will show you how to stream log events from LogDNA to [Splunk](https://www.splunk.com/). In order to achieve this, we will use the below two capabilities:\n- the streaming feature of logDNA that can be used to stream events to IBM Event Streams.\n- the Kafka Splunk connect to stream events from IBM Event Streams to Splunk.\n- Configure a Kafka Splunk Connect custom dashboard.\n\n\n### Create an instance of IBM Event Streams\n\nClick [here](https://cloud.ibm.com/catalog/services/event-streams) to create an instance of IBM Event Streams.\n\n![Create event streams](./images/create_event_streams.png \"Create event streams\")   \n\n### Create a topic on Event Streams\n\nOn the `Event Streams` console, click on `Manage` and then `Topics`.\n\n![Click create topic](./images/click_create_topic.png \"Click create topic\") \n\nEnter a topic name and click `Next`.\n\n![Click create topic](./images/click_create_topic.png \"Click create topic\") \n\nEnter the number of partitions and click `Next`.\n\n![Enter partitions](./images/enter_partitions.png \"Enter partitions\")\n\nSelect message retention time and click `Create Topic`.\n\n![Create topic](./images/create_topic.png \"Create topic\")\n\n### Note down credentials on Event Streams\n\nClick on `Service credentials`. Note down the `apikey` and `broker urls`.\n![Note credentials](./images/note_credentials.png \"Note credentials\")\n\n### Configure streaming on LogDNA\n\nOn the LogDNA Dashboard,navigate to the navigate to the gear icon (settings) > Streaming to enter the credentials gathered in the previous step as follows:\n\na.\tUsername = user   //always “token”\n\nb.\tPassword = api_key // apikey from Event Streams credentials.\nc.\tKafka URLs = kafka_brokers_sasl // Entered in on individual lines.\n\nd.\tEnter the name of a topic that we created earlier in event streams instance and hit \n“Save”.  \n\ne. Streaming may take up to 15 minutes to begin.\n\n\n![LogDNA Streams configuration](./images/logdna_streams_config.png \"LogDNA Streams Config\")\n\n### Set up Kafka Splunk Connect\n\n#### Download Apache Kafka\n\nDownload Apache Kafka [here](https://www.apache.org/dyn/closer.cgi?path=/kafka/2.5.0/kafka_2.13-2.5.0.tgz).\n\nCreate a folder `kafka_splunk_integration`. \n\nExtract the contents into the directory `kafka_splunk_integration`. \n\n#### Build Kafka Splunk jars\n\n1. Clone the repo from https://github.com/splunk/kafka-connect-splunk\n2. Verify that Java8 JRE or JDK is installed.\n3. Run mvn package. This will build the jar in the /target directory. The name will be splunk-kafka-connect-[VERSION].jar.\n\nCreate a folder `connector` in the directory `kafka_splunk_integration`. Copy the jar file to the `connector` folder.\n\n#### Modify connected-distributed.properties for Kafka connect\n\nDownload the `connected-distributed.properties` [here](https://github.com/IBM/cloud-enterprise-examples/blob/master/artifacts/logdna-splunk-integration/connected-distributed.propertes).\n- Edit connect-distributed.properties replacing the [BOOTSTRAP_SERVERS] and [APIKEY] placeholders with your Event Streams credentials.\n- Modify the `plugin.path` in the file contents to point to the directory `connector` we created in the previous step.\n- Now copy the file `connected-distributed.properties` to the `config` folder under the `kafka_2.13-2.5.0` folder.\n\n### Run Kafka connect\n\nOpen a terminal. Run the below commands. The [base dir] is the directory under which we created the folder `kafka_splunk_integration`.\n\n```\n$export KAFKA_HOME=[base dir]/kafka_splunk_integration/kafka_2.13-2.5.0\n$KAFKA_HOME/bin/connect-distributed.sh $KAFKA_HOME/config/connect-distributed.properties\n```\n\n### Install Splunk\n\nWe will install Splunk inside a container here. \nOpen a new terminal. Run the below commands.\n```\n$docker pull splunk/splunk:latest\n$docker run -d -p 8000:8000 -p 8088:8088 -e 'SPLUNK_START_ARGS=--accept-license' -e 'SPLUNK_PASSWORD=Test1234' splunk/splunk:latest\n```\nPlease check if the container is running successfully before moving to the next step.\n\n### Configure Splunk\n\nOpen the url - `http://localhost:8000` on a browser. The username is `admin` and password is `Test1234`.\n![Splunk Login](./images/splunk_login.png \"Splunk Login\")\n\n#### Create an index\n\nClick on `Settings` and then select `Indexes`.\n\n![Click indexes](./images/click_indexes.png \"Click indexes\") \n\nClick on `New Index`.\n![New Index](./images/new_index.png \"New Index\")  \n\nEnter a name say `logdnaindex` and click on `Save`.\n\n![Index details](./images/index_details.png \"Index details\")  \n\n\nThe index is now created. Make a note of the index name. We will need it to instantiate the connector.\n\n#### Create a token\n\nClick on `Settings` and then select `Data Input`.\n\n![Select data input](./images/select_data_input.png \"Select data input\")  \n\nClick on `Add New` to create a new `Http Event Collector`.\n\n![Add new HEC](./images/add_new_hec.png \"Add new HEC\")\n\nEnter a name say `logdnatoken` and select `Enable Indexer`. Click on `Next`.\n\n![Enter token details](./images/enter_token_details.png \"Enter token details\")  \n\nSelect the index we created earlier and click `Review`.\n\n![Select HEC index](./images/select_hec_index.png \"Select HEC Index\")\n\nClick `Submit`.\n\n![Click token submit](./images/click_token_submit.png \"Click token submit\")\n\nCopy the created token. We will need it to instantiate the connector.\n\n![Copy token](./images/copy_token.png \"Copy token\")  \n\n### Create an instance of Kafka Splunk connector\n\nOpen a new terminal. Run the below command after specifying the index name and token.\n```\n$curl localhost:8083/connectors -X POST -H \"Content-Type: application/json\" -d '{\n   \"name\": \"kafka-connect-splunk\",\n   \"config\": {\n     \"connector.class\": \"com.splunk.kafka.connect.SplunkSinkConnector\",\n     \"tasks.max\": \"3\",\n     \"splunk.indexes\": \"[index name]\",\n     \"topics\":\"logdnatopic\",\n     \"splunk.hec.uri\": \"http://localhost:8088\",\n     \"splunk.hec.token\": \"[token]\"\n   }\n }'\n ```\n \n ### View the log data\n \n #### Create report\n \n Click `Settings` and select `Searches, Reports, and Alerts`.\n \n ![Select reports](./images/select_reports.png \"Select reports\") \n \n Click `New Report`.\n \n![New report](./images/new_report.png \"New Report\") \n\nEnter `Title`, `Search` criteria and click on `Save`.\n\n![Report details](./images/report_details.png \"Report details\")\n\n#### Run the report and view data\n\n![View data](./images/view_data.png \"View data\")  \n \n### Configure a Kafka Splunk Connect custom dashboard\nGo to `Settings` and choose `Searches, reports, and alerts`.\n\n![Create dashboard](./images/create_dashboard2.png \"Create dashboard\")  \n\nChoose the report you wish to run, similar to the screen below.  The `Gsi Logdna` report is used in this screenshot\n\n![Create dashboard](./images/create_dashboard3d.png \"Create dashboard\") \n\nClick on the `Dashboards` icon, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard4.png \"Create dashboard\") \n\nClick on the `Create New Dashboard` icon, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard5.png \"Create dashboard\")\n\nProvide a `Title` for your dashboard and set `Permissions`, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard6.png \"Create dashboard\")\n\nClick the `+ Add Panel` button, then choose from the `Add Panel` menu,`Messages by minute last 3 hours`, shown below. \n\n![Create dashboard](./images/create_dashboard8.png \"Create dashboard\")\n\nNext `Select Visualization` and click on the `Edit Drilldown` icon and choose `Trellis`. \n\nYou can also add a name to the `No title` section, **similar to the screen shown below**. \n\n![Create dashboard](./images/create_dashboard10e.png \"Create dashboard\")\n\nSelect `Use Trellis Layout`, choose `Size`, `Scale`, and select `Independent`.\n\n![Create dashboard](./images/create_dashboard11.png \"Create dashboard\")\n\nThe `Select Visualization` `Pie Chart` is shown below for the **Messages by minute last 3 hours** Report.\n\n![Create dashboard](./images/create_dashboard13c.png \"Create dashboard\")\n\nShown below is the `Select Visualization` `Line Chart` for the **Messages by minute last 3 hours** Report.\n\n![Create dashboard](./images/create_dashboard15b.png \"Create dashboard\")\n\nYou can configure a Splunk dashboard with multiple event data types, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard16.png \"Create dashboard\")\n\nAfter configuring the desired dashboard, click `Save`. Your dashboard should show up saved, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard17.png \"Create dashboard\")\n\n<InlineNotification>\n\n**For more information on configuring Splunk Dashboards**, **see** [Dashboards and Visualizations](https://docs.splunk.com/Documentation/Splunk/8.0.4/Viz/DashboardEditor), [Create dashboards & panels](https://docs.splunk.com/Documentation/Splunk/8.0.4/SearchTutorial/Createnewdashboard) **and** [Splunk application for Kafka Smart Monitoring](https://telegraf-kafka.readthedocs.io/en/latest/)\n\n</InlineNotification>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"Mdx","contentDigest":"d56e9ed7edad88fd975ba99743715d3b","counter":758,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Configure Streaming for Third Party Tools","description":"How does Log Streaming work & how to leverage it","keywords":"ibm cloud"},"exports":{},"rawBody":"---\ntitle: Configure Streaming for Third Party Tools\ndescription: How does Log Streaming work & how to leverage it\nkeywords: 'ibm cloud'\n---\n\nIn this document, We will show you how to stream log events from LogDNA to [Splunk](https://www.splunk.com/). In order to achieve this, we will use the below two capabilities:\n- the streaming feature of logDNA that can be used to stream events to IBM Event Streams.\n- the Kafka Splunk connect to stream events from IBM Event Streams to Splunk.\n- Configure a Kafka Splunk Connect custom dashboard.\n\n\n### Create an instance of IBM Event Streams\n\nClick [here](https://cloud.ibm.com/catalog/services/event-streams) to create an instance of IBM Event Streams.\n\n![Create event streams](./images/create_event_streams.png \"Create event streams\")   \n\n### Create a topic on Event Streams\n\nOn the `Event Streams` console, click on `Manage` and then `Topics`.\n\n![Click create topic](./images/click_create_topic.png \"Click create topic\") \n\nEnter a topic name and click `Next`.\n\n![Click create topic](./images/click_create_topic.png \"Click create topic\") \n\nEnter the number of partitions and click `Next`.\n\n![Enter partitions](./images/enter_partitions.png \"Enter partitions\")\n\nSelect message retention time and click `Create Topic`.\n\n![Create topic](./images/create_topic.png \"Create topic\")\n\n### Note down credentials on Event Streams\n\nClick on `Service credentials`. Note down the `apikey` and `broker urls`.\n![Note credentials](./images/note_credentials.png \"Note credentials\")\n\n### Configure streaming on LogDNA\n\nOn the LogDNA Dashboard,navigate to the navigate to the gear icon (settings) > Streaming to enter the credentials gathered in the previous step as follows:\n\na.\tUsername = user   //always “token”\n\nb.\tPassword = api_key // apikey from Event Streams credentials.\nc.\tKafka URLs = kafka_brokers_sasl // Entered in on individual lines.\n\nd.\tEnter the name of a topic that we created earlier in event streams instance and hit \n“Save”.  \n\ne. Streaming may take up to 15 minutes to begin.\n\n\n![LogDNA Streams configuration](./images/logdna_streams_config.png \"LogDNA Streams Config\")\n\n### Set up Kafka Splunk Connect\n\n#### Download Apache Kafka\n\nDownload Apache Kafka [here](https://www.apache.org/dyn/closer.cgi?path=/kafka/2.5.0/kafka_2.13-2.5.0.tgz).\n\nCreate a folder `kafka_splunk_integration`. \n\nExtract the contents into the directory `kafka_splunk_integration`. \n\n#### Build Kafka Splunk jars\n\n1. Clone the repo from https://github.com/splunk/kafka-connect-splunk\n2. Verify that Java8 JRE or JDK is installed.\n3. Run mvn package. This will build the jar in the /target directory. The name will be splunk-kafka-connect-[VERSION].jar.\n\nCreate a folder `connector` in the directory `kafka_splunk_integration`. Copy the jar file to the `connector` folder.\n\n#### Modify connected-distributed.properties for Kafka connect\n\nDownload the `connected-distributed.properties` [here](https://github.com/IBM/cloud-enterprise-examples/blob/master/artifacts/logdna-splunk-integration/connected-distributed.propertes).\n- Edit connect-distributed.properties replacing the [BOOTSTRAP_SERVERS] and [APIKEY] placeholders with your Event Streams credentials.\n- Modify the `plugin.path` in the file contents to point to the directory `connector` we created in the previous step.\n- Now copy the file `connected-distributed.properties` to the `config` folder under the `kafka_2.13-2.5.0` folder.\n\n### Run Kafka connect\n\nOpen a terminal. Run the below commands. The [base dir] is the directory under which we created the folder `kafka_splunk_integration`.\n\n```\n$export KAFKA_HOME=[base dir]/kafka_splunk_integration/kafka_2.13-2.5.0\n$KAFKA_HOME/bin/connect-distributed.sh $KAFKA_HOME/config/connect-distributed.properties\n```\n\n### Install Splunk\n\nWe will install Splunk inside a container here. \nOpen a new terminal. Run the below commands.\n```\n$docker pull splunk/splunk:latest\n$docker run -d -p 8000:8000 -p 8088:8088 -e 'SPLUNK_START_ARGS=--accept-license' -e 'SPLUNK_PASSWORD=Test1234' splunk/splunk:latest\n```\nPlease check if the container is running successfully before moving to the next step.\n\n### Configure Splunk\n\nOpen the url - `http://localhost:8000` on a browser. The username is `admin` and password is `Test1234`.\n![Splunk Login](./images/splunk_login.png \"Splunk Login\")\n\n#### Create an index\n\nClick on `Settings` and then select `Indexes`.\n\n![Click indexes](./images/click_indexes.png \"Click indexes\") \n\nClick on `New Index`.\n![New Index](./images/new_index.png \"New Index\")  \n\nEnter a name say `logdnaindex` and click on `Save`.\n\n![Index details](./images/index_details.png \"Index details\")  \n\n\nThe index is now created. Make a note of the index name. We will need it to instantiate the connector.\n\n#### Create a token\n\nClick on `Settings` and then select `Data Input`.\n\n![Select data input](./images/select_data_input.png \"Select data input\")  \n\nClick on `Add New` to create a new `Http Event Collector`.\n\n![Add new HEC](./images/add_new_hec.png \"Add new HEC\")\n\nEnter a name say `logdnatoken` and select `Enable Indexer`. Click on `Next`.\n\n![Enter token details](./images/enter_token_details.png \"Enter token details\")  \n\nSelect the index we created earlier and click `Review`.\n\n![Select HEC index](./images/select_hec_index.png \"Select HEC Index\")\n\nClick `Submit`.\n\n![Click token submit](./images/click_token_submit.png \"Click token submit\")\n\nCopy the created token. We will need it to instantiate the connector.\n\n![Copy token](./images/copy_token.png \"Copy token\")  \n\n### Create an instance of Kafka Splunk connector\n\nOpen a new terminal. Run the below command after specifying the index name and token.\n```\n$curl localhost:8083/connectors -X POST -H \"Content-Type: application/json\" -d '{\n   \"name\": \"kafka-connect-splunk\",\n   \"config\": {\n     \"connector.class\": \"com.splunk.kafka.connect.SplunkSinkConnector\",\n     \"tasks.max\": \"3\",\n     \"splunk.indexes\": \"[index name]\",\n     \"topics\":\"logdnatopic\",\n     \"splunk.hec.uri\": \"http://localhost:8088\",\n     \"splunk.hec.token\": \"[token]\"\n   }\n }'\n ```\n \n ### View the log data\n \n #### Create report\n \n Click `Settings` and select `Searches, Reports, and Alerts`.\n \n ![Select reports](./images/select_reports.png \"Select reports\") \n \n Click `New Report`.\n \n![New report](./images/new_report.png \"New Report\") \n\nEnter `Title`, `Search` criteria and click on `Save`.\n\n![Report details](./images/report_details.png \"Report details\")\n\n#### Run the report and view data\n\n![View data](./images/view_data.png \"View data\")  \n \n### Configure a Kafka Splunk Connect custom dashboard\nGo to `Settings` and choose `Searches, reports, and alerts`.\n\n![Create dashboard](./images/create_dashboard2.png \"Create dashboard\")  \n\nChoose the report you wish to run, similar to the screen below.  The `Gsi Logdna` report is used in this screenshot\n\n![Create dashboard](./images/create_dashboard3d.png \"Create dashboard\") \n\nClick on the `Dashboards` icon, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard4.png \"Create dashboard\") \n\nClick on the `Create New Dashboard` icon, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard5.png \"Create dashboard\")\n\nProvide a `Title` for your dashboard and set `Permissions`, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard6.png \"Create dashboard\")\n\nClick the `+ Add Panel` button, then choose from the `Add Panel` menu,`Messages by minute last 3 hours`, shown below. \n\n![Create dashboard](./images/create_dashboard8.png \"Create dashboard\")\n\nNext `Select Visualization` and click on the `Edit Drilldown` icon and choose `Trellis`. \n\nYou can also add a name to the `No title` section, **similar to the screen shown below**. \n\n![Create dashboard](./images/create_dashboard10e.png \"Create dashboard\")\n\nSelect `Use Trellis Layout`, choose `Size`, `Scale`, and select `Independent`.\n\n![Create dashboard](./images/create_dashboard11.png \"Create dashboard\")\n\nThe `Select Visualization` `Pie Chart` is shown below for the **Messages by minute last 3 hours** Report.\n\n![Create dashboard](./images/create_dashboard13c.png \"Create dashboard\")\n\nShown below is the `Select Visualization` `Line Chart` for the **Messages by minute last 3 hours** Report.\n\n![Create dashboard](./images/create_dashboard15b.png \"Create dashboard\")\n\nYou can configure a Splunk dashboard with multiple event data types, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard16.png \"Create dashboard\")\n\nAfter configuring the desired dashboard, click `Save`. Your dashboard should show up saved, similar to the screen shown below. \n\n![Create dashboard](./images/create_dashboard17.png \"Create dashboard\")\n\n<InlineNotification>\n\n**For more information on configuring Splunk Dashboards**, **see** [Dashboards and Visualizations](https://docs.splunk.com/Documentation/Splunk/8.0.4/Viz/DashboardEditor), [Create dashboards & panels](https://docs.splunk.com/Documentation/Splunk/8.0.4/SearchTutorial/Createnewdashboard) **and** [Splunk application for Kafka Smart Monitoring](https://telegraf-kafka.readthedocs.io/en/latest/)\n\n</InlineNotification>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","fileAbsolutePath":"/Users/isaias/Documents/Projects/Developer_Advocate_Group/CODE_PATTERNS/ibm-asean-enterprise-cloud-patterns/src/pages/log-streaming/configure-streaming-for-third-party-tools/index.mdx"}}}}